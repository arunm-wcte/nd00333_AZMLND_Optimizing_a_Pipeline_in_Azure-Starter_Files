# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains data about a marketing campaign conducted by a Portuguese banking institution. The campaign primarily used phone calls to contact potential customers and promote a bank term deposit (a savings product).

We seek to predict whether a customer will subscribe to the term deposit based on various features. This is a binary classification problem, where the target variable indicates whether a customer subscribed (yes) or did not subscribe (no). 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best-performing model was the Voting Ensemble model. This model was identified during the AutoML run. The model gave a classification accuracy of 0.9176.  

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The machine learning pipeline is designed to train a logistic regression model for predicting customer subscriptions to a bank term deposit. The pipeline involves several key components: data preparation, hyperparameter tuning, and model training. Below is an explanation of the architecture and its components:

Data: The pipeline begins with loading and cleaning the dataset, transforming categorical variables into numerical format, and preparing the data for model training.

Hyperparameter Tuning: The RandomParameterSampling and BanditPolicy components are designed to tune the model's hyperparameters efficiently, enhancing the model's performance based on the provided parameters. Ther hyperparameters tuned are the number of iterations and regularization strength.

Classification Algorithm: The pipeline employs the logistic regression algorithm for classification, which aims to predict whether a customer will subscribe to a term deposit based on their features.

**What are the benefits of the parameter sampler you chose?**

The parameter sampler chosen was RandomParameterSampling. Using RandomParameterSampling provides a balanced approach to hyperparameter tuning, allowing for efficient exploration of the hyperparameter space while avoiding the limitations of grid-based methods. Itâ€™s particularly beneficial when the relationship between hyperparameters and model performance is complex or non-linear.

**What are the benefits of the early-stopping policy you chose?**

The early stopping policy chosen was BanditPolicy. BanditPolicy in Azure Machine Learning HyperDrive is designed to optimize hyperparameter tuning by using an adaptive strategy that evaluates the performance of runs in real time. This policy monitors the performance of ongoing trials and compares them to a set threshold, allowing it to terminate poorly performing runs early. The key idea behind this approach is to focus resources on the most promising hyperparameter configurations, which leads to faster convergence on optimal parameters while conserving computational resources.

The benefits of using BanditPolicy include increased efficiency in the training process, as it reduces the overall time and computational cost by stopping ineffective trials promptly. Additionally, it promotes better model performance by enabling the algorithm to adaptively learn from previous trials, ensuring that resources are allocated to configurations that demonstrate the highest potential. This approach not only enhances the exploration of the hyperparameter space but also helps mitigate the risk of overfitting by preventing extended training on suboptimal models.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The AutoML run is performed for a classification task using the specified configuration in AutoMLConfig. The AutoML experiment utilizes accuracy as the primary metric, and performs 5 cross-validations on the provided dataset.
During the AutoML run, various models (like logistic regression, decision trees, etc.) are trained, and hyperparameters specific to each model are automatically tuned. The best-performing model and its hyperparameters are determined based on the specified primary metric (accuracy) and can be retrieved after the run completes. The best model obtained using AutoML was MaxAbsScaler Light Gradient Boosting Machine. Some of the hyperparameters associated with the model are boosting type, number of leaves, max depth, etc.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The Hyperparameter tuning performed using hyperdrive used only one machine learning model (Logistic Regression), whereas AutoML explored a plethora of models. AutoML also tuned the hyperparameters associated with each of these different models. Thus AutoML allows us to identify the best model in a highly comprehensive manner. The best model obtained using AutoML was MaxAbsScaler Light Gradient Boosting Machine, and the accuracy obtained using the said model was 0.9151. The accuracy obtained using the Logistic Regression model tuned using hyperdrive hyperparameter tuning was 0.9088.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

In the hyperparameter tuning pipeline, Bayesian parameter sampling could be used instead of random sampling. Bayesian sampling is more efficient than random sampling because it uses prior knowledge of past evaluations to select the next set of hyperparameters.
A more robust performance metric such as AUC_weighted or F1_score could be used to evaluate the models.
The experiment time could be increased to enable in-depth exploration.
The number of cross-validations can be increased to evaluate the model better.

